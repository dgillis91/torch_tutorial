{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, AutoGrad is the mechanism used to perform\n",
    "the gradient computation in back propogation. To do this,\n",
    "torch keeps track of gradients in the \"compute graph\". Torch\n",
    "has a mechanism to identify which things to keep track of. \n",
    "To do this, we specify whether to track gradients of operations\n",
    "on given tensors. \n",
    "\n",
    "The following code creates a tensor for which we track gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] x:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((2, 2), requires_grad=True)\n",
    "print(f'[+] x:\\n{x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also Functions in torch. Functions and tensors are what make up the\n",
    "computation graph. All Tensors, save those that are allocated by the user, \n",
    "have a function that generates them. Seemingly this is the same as an op in\n",
    "Tensorflow. Though, it's worth noting that tensorflow actually store gradient\n",
    "operations in the graph. For more info on that see uhh . . .\n",
    "[Ian Goodfellow's Book](https://www.amazon.com/Deep-Learning-NONE-Ian-Goodfellow-ebook/dp/B01MRVFGX4/ref=sr_1_3?keywords=deep+learning&qid=1565836699&s=gateway&sr=8-3).\n",
    "It's a tough text.\n",
    "\n",
    "I digress. The function creating the tensor can be accessed through `.grad_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] y:\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(f'[+] y:\\n{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, allocated tensors don't require gradients. However, if\n",
    "a there is a single input to an operation that requires a gradient, \n",
    "the output will also require gradients. But, if all inputs don't\n",
    "require it, the output will not. Backward computation is not performed\n",
    "on subgraphs that do not require gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] x.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((2, 2))\n",
    "\n",
    "print(f'[+] x.requires_grad: {x.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] a.requires_grad: False\n",
      "[+] b.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# X & Y do not require gradients, but Z does. \n",
    "x = torch.randn((5, 5))\n",
    "y = torch.randn((5, 5))\n",
    "z = torch.randn((5, 5), requires_grad=True)\n",
    "\n",
    "# a will not require gradients because the inputs do not\n",
    "a = x + y\n",
    "print(f'[+] a.requires_grad: {a.requires_grad}')\n",
    "\n",
    "# b will require gradients becaus z does.\n",
    "b = a + z\n",
    "print(f'[+] b.requires_grad: {b.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be high level. For more details see this \n",
    "[link](https://pytorch.org/docs/stable/notes/autograd.html?highlight=grad_fn).\n",
    "\n",
    "AutoGrad is a reverse automatic differentiation system. We have a graph of \n",
    "Function objects which can be applied to perform the forward pass. That is, \n",
    "we have the root of the graph as inputs, and leave as outputs. As we perform\n",
    "the forward pass, we build up a graph of the function computing the gradients.\n",
    "Once the forward pass is completed, we can use this gradient graph (and the\n",
    "chain rule) to compute the backward pass. \n",
    "\n",
    "Interestingly, the gradient graph is computed at each iteration. I'm near\n",
    "positive this is not the case in Tensorflow. This is what allows for dynamic\n",
    "control flow. But, it likely comes at a performance cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing tracing history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to remove tracking history, or to temporarily\n",
    "stop tracking. First, if you call `.detach()`, tracking history is removed\n",
    "entirely. Additionally, wrapping computations in \n",
    "\n",
    "```\n",
    "with torch.no_grad():\n",
    "    ...\n",
    "```\n",
    "\n",
    "prevents tracking history in the context manager.\n",
    "\n",
    "Last, you can also call `.requires_grad_()` to change it in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] b.requires_grad: False\n",
      "[+] c.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "b = b.detach()\n",
    "print(f'[+] b.requires_grad: {b.requires_grad}')\n",
    "\n",
    "# This will raise an error, as b is not a leaf.\n",
    "try:\n",
    "    b.requires_grad_(False)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "    \n",
    "# c won't require a gradient, even though z does.\n",
    "with torch.no_grad():\n",
    "    c = x + z\n",
    "print(f'[+] c.requires_grad: {c.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously, as we dynamically build (and compute) our\n",
    "compute graph, torch builds a gradient graph which can be used for\n",
    "gradient computation by the chain rule. We create a tensor, and \n",
    "perform operations on it:\n",
    "\n",
    "$$y = x + 2$$\n",
    "\n",
    "$$z = y \\circ y \\circ 3$$\n",
    "\n",
    "Where $\\circ$ is defined as the \n",
    "[Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))\n",
    "\n",
    "$$out = E[z]$$\n",
    "\n",
    "We then perform the backward pass, which gives us access to gradients\n",
    "of the individual operations. For example, taking `x.grad` gives us\n",
    "\n",
    "$$\\frac{\\partial out}{\\partial x}$$\n",
    "\n",
    "I'm not going to write the math out here, but I highly encourage \n",
    "anyone to read the end of the tutorial this notebook is based on. \n",
    "Torch is computing the Jacobian matrix. This is an incredibly important\n",
    "thing to understand if you wish to follow the mathematics of deep learning. \n",
    "\n",
    "The Jacobian matrix tells us what linear transformation a non linear \n",
    "transformation looks like, in the neighborhood of a point. Because \n",
    "neural networks are applying many non-linear transformations, this\n",
    "is an important result.\n",
    "\n",
    "The last reference is by a fellow named Christopher Olah. Likely the\n",
    "most recommended of this list.\n",
    "\n",
    "References:\n",
    "\n",
    "[Khan Academy Jacobian](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/the-jacobian-matrix)\n",
    "\n",
    "[Jacobian Definition](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)\n",
    "\n",
    "[Torch AutoGrad Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "\n",
    "[Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] y.grad_fn: <AddBackward0 object at 0x7f2804576390>\n",
      "[+] z:\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "[+] out:\n",
      "27.0\n",
      "[+] x.grad:\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((2, 2), requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "print(f'[+] y.grad_fn: {y.grad_fn}')\n",
    "\n",
    "z = y * y * 3\n",
    "print(f'[+] z:\\n{z}')\n",
    "out = z.mean()\n",
    "print(f'[+] out:\\n{out}')\n",
    "\n",
    "out.backward()\n",
    "\n",
    "print(f'[+] x.grad:\\n{x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
