{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to start with MNIST, because it's a dataset that I'm\n",
    "massively familiar with that. On top of that, it has a continuous\n",
    "distribution, so once I get comfortable with implementing a simple\n",
    "convolutional neural network, I'll develop an auto encoder, and a \n",
    "variational auto encoder. \n",
    "\n",
    "For this example, we will do a simple convolutional neural network\n",
    "in torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchVision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [torchvision](https://pytorch.org/docs/stable/torchvision/index.html) \n",
    "package contains utilities for many tasks in computer vision. \n",
    "These include transformations, loading toy data sets, and popular model\n",
    "architectures. In this tutorial, I don't intend to use the last. I'll just \n",
    "[load](http://127.0.0.1:8888/?token=755449bfdb2fd6b486d94057b9759d4c877749c8b9a71482)\n",
    "MNIST (a popular dataset of images of numbers) and play about with it. \n",
    "\n",
    "In addition, we will start by just training the model without transformations. While\n",
    "this tutorial focuses on models in torch, it should be fun to see how transforms impact\n",
    "MNIST. Up front, I'll hypothesize that any gains will be very much marginal. The MNIST\n",
    "data set is pretty free of noise when preprocessed properly. Worth noting, there are \n",
    "plenty of datasets to play with, including QMNIST, which is not processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, we won't do any elaborate preprocessing. Instead, we just load the data\n",
    "and normalize.\n",
    "\n",
    "The image datasets are loaded as PILImages. We need to load these as tensors, and\n",
    "normalize them. For this, we can apply `transformation`s from `torchvision`.\n",
    "These \n",
    "[transforms](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision-transforms)\n",
    "are similar to the sklearn transforms, but are specifically for images. There is\n",
    "also a funcitonal module that allows for more custom control. \n",
    "\n",
    "Users can call the `Compose()` method to build pipelines of transformations. We \n",
    "will just be casting the images to tensors, then centering the data ourselves. \n",
    "This way, we don't have to make any underlying assumptions about the distribution \n",
    "of the data. Worth noting that `Normalize` always expects an iterable. If you need\n",
    "to normalize single channel images, you must pass an iterable with one element. \n",
    "\n",
    "Note that mnist comes with a single (gray) channel of 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "trainset = MNIST(\n",
    "    root='./data/', train=True, \n",
    "    download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's possible to add transforms to a datasets transforms after the\n",
    "dataset has been constructed. This follows logically from how `Compose` is created.\n",
    "The `Compose` class is [defined](https://github.com/pytorch/vision/blob/19315e313511fead3597e23075552255d07fcb2a/torchvision/transforms/transforms.py#L43)\n",
    "with:\n",
    "\n",
    "```\n",
    "def __init__(self, transforms: list):\n",
    "    '''\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    '''\n",
    "    self.transforms = transforms\n",
    "```\n",
    "\n",
    "Because of this, we are free to append transformations to this object. The trainset\n",
    "that uses this will reference that object. \n",
    "\n",
    "So, in this case, we compute the mean and standard deviation so we can normalize the\n",
    "images with the parameters that define their distribution. We then `append` the \n",
    "created transformation to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = trainset.data.float().mean() / 255\n",
    "sigma = trainset.data.float().std() / 255\n",
    "\n",
    "transform.transforms.append(\n",
    "    transforms.Normalize(\n",
    "        mean=(mu,), std=(sigma,)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(tensor(0.1307),), std=(tensor(0.3081),))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataLoader` class is used to . . . well . . . load data. And it's\n",
    "pretty freakin' swell. First, it is multithreaded, and you can specify\n",
    "the worker count. That's really very brilliant, and much simpler (and\n",
    "more elegant) than the paradigm of using generators. \n",
    "\n",
    "The `__init__` \n",
    "[method](https://github.com/pytorch/pytorch/blob/10c456417ce49cb5bfad8aba2bd7e3a23a83aef2/torch/utils/data/dataloader.py#L120) \n",
    "takes in many arguments, but one is the dataset, in this case `trainset`.\n",
    "We can then create a python iterator over that dataset. All of the\n",
    "transformations will happen from the `transforms` in the dataset. That is,\n",
    "the dataset handles applying the transform when you call \n",
    "`__getitem__(self, idx)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=2\n",
    ")\n",
    "targets = np.array([x for x in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "im, lab = dataiter.next()\n",
    "\n",
    "im.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1 6 5 4\n"
     ]
    }
   ],
   "source": [
    "SHOW_COUNT = 5\n",
    "%matplotlib inline\n",
    "def image_show(image):\n",
    "    np_image = image.numpy()\n",
    "    plt.imshow(np.transpose(np_image, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "image_show(\n",
    "    torchvision.utils.make_grid(im[0:SHOW_COUNT, :, :, :])\n",
    ")\n",
    "print(' '.join(str(d) for d in targets[lab[:SHOW_COUNT]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth testing at this point is whether adding a transformation will affect \n",
    "the data loader. Based on reading the source, the dataset in the data loader\n",
    "is just a reference. So, if we add a transform, the dataloader will just load\n",
    "data with the transform applied.\n",
    "\n",
    "This does, in fact, work. In addition, it appears that the transforms are\n",
    "applied in the order of the list. So, we have to call `insert` to append to\n",
    "the front of the list.\n",
    "\n",
    "Details on the `RandomRotation` transform \n",
    "[here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomRotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABoCAYAAADo66t9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAOe0lEQVR4nO3df4wUZZ7H8ff3EDiXzcl6cwICiqugqxtdJ/7iREPkVoSQ2Rg8o1kRFeMfcgpm9YQjEc7E3ycu5pCTCEc0ZHfPwTsJiriymGhUVvwBusgAuuzyQxFyK2u8GDH7vT+6qunpru7pH9XVVc3nlUym+qmaqm8/XfXMt56qetrcHRERyZ6/anUAIiJSHzXgIiIZpQZcRCSj1ICLiGSUGnARkYxSAy4iklENNeBmdoWZ9ZjZTjObE1dQIiLSN6v3PnAz6wdsB34M7AHeBq51963xhSciIuUc08DfXgDsdPdPAMzsl8BPgLINuJnpqSERkdoddPe/Ky5spAtlOLC74PWeoKwXM7vFzDaZ2aYGtiUicjT7Q1RhIxm4RZSVZNjuvhRYCsrARUTi1EgGvgcYWfB6BLCvsXBERKRajTTgbwOjzewUMxsAXAOsjicsERHpS91dKO7+rZn9E7AO6Acsd/ffxRaZiIhUVPdthHVtTH3gIiL1eMfdzysu1JOY0namTZvGtGnTWh2GSNOpARcRyahGbiMUSZXly5cDcOONNwIwcuSRm6Tuv//+lsQk0kzKwEVEMkoXMaVtVNqXzaKeOxPJDF3EFBFpJ0ddH/iSJUsA6OrqAuDEE0/Mz9u3L/cg6ZgxYwD46quvEo5OGnHJJZcA8Nprr5XM6+zsBODdd99NNKZ2EJ7Z6CwmfZSBi4hklBpwEZGMavuLmFOnTgWgu7u75r+96qqrAFi1alWsMUlzRe3TOv2vXXE9vvfee8CR7ihJlC5iioi0k7bKwA8cOADAaaedli/74osvGl5vWrO3hx9+GIC77roLSG+cSdFthPEqV5/Dhg3LT3/22WdJhXO0UwYuItJO2uI2wv79+wPQ0dEBVJd1T548GYC1a9eWzCvOPNJwG9Xll1+en163bl3kMkd732/4XqPqIRzc6plnnkk0pnY0ZMiQ/LQy8NZSBi4iklFt0Qdey3uoJSMtXu/+/fvz00OHDq16PXGI63OqdObRl7feeis/feGFFwLpzPCj6uqFF14AYMqUKUmHkwmjR48uKdu+fXvksmn8zKH0THnBggX5efPnzwfg2GOPBeDSSy/t9bdPPfVUfvqkk05qZpj1Uh+4iEg7yWQGXkvMDz74IABz586NdTtJZyFRsTz77LMAXH311WWX6cvgwYPz04cOHao5hlAas7Kj/ZrArbfemp9+4oknes3bvHkzAGeffTYA48ePz8979dVXI9eX9rqLqy2bOHEiUHqtqcXvXxm4iEg7UQMuIpJRbdWFEvcpTvF2tm7dmp8+66yzYt1WOXfffTdwpCuo0KmnngrAJ5980qv8uOOOy0/X8iDTSy+9BMCkSZMi51ezr6ThNPucc84B4P3338+XNRJX1PsOv/VnxYoVda+3WV5//XUALr744sS2efPNNwO9LwYWa/a+kWRbFvryyy+BI8NuvPzyy83alLpQRETaSWYy8AsuuCA/vXHjxnLrr3f1kdJw0a5SDHfeeScAjz76aJ/rqZTJl1P8HrOSgcf1uVXzfsNMPNTKjLwVGWgjmnW8Fl/cj1om1NPTU7LM6aefHltMW7ZsyU+HQ17UmaUrAxcRaSeZycALJdUH/sADDwAwZ86cknkzZ84ESm/Papa4zwZq+dzDvvaPP/647DK1nA0024wZM4Do/thq6qqeYyINZx4TJkwA4JVXXmlxJNW5/vrrgXQMb7Bt27aSsn79+gG9B8drRHh7YqjGTFwZuIhIO8lkBr53716g9/dZBuuPY/V5lTLwZm2znEqf08qVKwG47rrral7v2LFj89NvvPFG7YEF0pSBh2o5a1m+fHl+urhfOzRgwID89DfffFNxfWlTbkC2Wo7/hQsXAr2Ph8OHD0euJw13pcTltttuA+Dxxx8vmRf2oZ9xxhlA84b1QBm4iEh7yeRwssOHDwdK/9uF3yI/aNCgWLYTPn5fKQNPyj333JOfvvfee2Nb75tvvpmfDh+rj+NLMLLmpptuyk+Xy8DDbDNKGoYcrqRcXJWG4O3rb6tZZtmyZVVEl11h5h3asGEDUNo7AEfubolzH1EGLiKSUWrARUQyKpMXMUMnn3wyALt27SreTpybScUDPYXKxdPIxcxClW7D60sauhDC/aJQuI+Ui6+azzht+0Gc2vm9NarSRcxa6iYci37NmjX1hKGLmCIi7STTGXio+D2EmSg0no1Grb9QK7KT8Psdn3766cj5ccVU/K331UhTtlbL59bocZCm912LtO3bcYq6RTYcfGr37t29lo0anK5c5t2ielEGLiLSTvrMwM1sJPA0MBT4C7DU3ReZ2fHAr4BRwC7ganf/Ux/ramq6HzWMaBz9wmnNUmbPng3AY4891qt806ZN+enzzz8/tu1lZTCrUL2fWxMfxkidtO7bcauinSsp+/rrrwEYOHBgn8smoO4M/FvgZ+7+A+AiYKaZnQnMAda7+2hgffBaREQSUnMfuJk9D/x78DPe3T81s2HAq+5ecRzGZmfgUe8lzEZryUQrXXUOpSk7KX7f4cMEAJdddlnTthMlTfVSqK8B0ArnP/LII0Dlvv+0vs9aVfpMwwe7+vqu1CyoZzjlctKUgdf0JKaZjQLOBTYCQ9z9U4CgET+hzN/cAtxSa7QiIlJZ1Q24mX0XWAXMdvc/V/tfyN2XAkuDdWRrxHkRkRSrqgE3s/7kGu+V7v5cULzfzIYVdKF83qwgG3HeebmzjrVr1wLlv++x0JVXXhlZntbT5jFjxgCwY8eOpm5n3759+emosR7SrNzoeGEXwn333ZcvKxx1sFBaP/9FixYBcPvtt/cqL4y3eN+oZozrxYsXA/HcittqDz30END7G3IAXnzxxVaEE5s+L2Jabi9YBnzk7gsLZq0GpgfT04Hn4w9PRETKqeY2wnHAa8AH5G4jBPgXcv3g/wWcBPwR+Ed3/98+1pVIF0q9F9v6eoQ8rRlYKyT1rUhxy+q37URp9kN4aX3fccrQLaP1XcR099eBcpFPaDQqERGpTybHA+/LHXfckZ8ufsgllLVv8E6ruAbQSkqYRU2dOhWA7u7ukmXCvv609/NXM8iWVBaVVY8bNw6AefPmAdDV1ZVoTLXQo/QiIhnVFoNZVbJgwQIA5s+fX/c6wgd6Zs2aFUdIIk11tA7M1eY0mJWISDtp+ww8dMMNNwBHHpPu6OgoWebgwYMAPPnkkwBMnjwZgM7OzgQiFBEpSxm4iEg7UQMuIpJRR00XiohIhqkLRUSknagBFxHJKDXgIiIZpQZcRCSj1ICLiGSUGnARkYxSAy4iklFqwEVEMkoNuIhIRqkBFxHJKDXgIiIZpQZcRCSjkv5OzIPAV8HvrOhA8TaT4m0uxdtcScV7clRhoqMRApjZpqhRtdJK8TaX4m0uxdtcrY5XXSgiIhmlBlxEJKNa0YAvbcE2G6F4m0vxNpfiba6Wxpt4H7iIiMRDXSgiIhmlBlxEJKMSa8DN7Aoz6zGznWY2J6ntVsvMRprZBjP7yMx+Z2azgvLjzezXZrYj+P29VsdayMz6mdl7ZrYmeH2KmW0M4v2VmQ1odYyFzGywmXWb2bagrsemuY7N7I5gf/jQzH5hZn+dpjo2s+Vm9rmZfVhQFlmflvN4cAxuMbPOlMT7SLA/bDGz/zazwQXz5gbx9pjZxDTEWzDvTjNzM+sIXidev4k04GbWD1gMTALOBK41szOT2HYNvgV+5u4/AC4CZgYxzgHWu/toYH3wOk1mAR8VvH4IeCyI90/AjJZEVd4i4CV3PwM4h1zsqaxjMxsO3A6c5+4/BPoB15CuOl4BXFFUVq4+JwGjg59bgCUJxVhoBaXx/hr4obufDWwH5gIEx981wFnB3zwRtCVJWkFpvJjZSODHwB8LipOvX3dv+g8wFlhX8HouMDeJbTcQ8/PBB9QDDAvKhgE9rY6tIMYR5A7Qy4A1gJF7KuyYqHpv9Q/wN8DvCS6eF5Snso6B4cBu4HhyTy2vASamrY6BUcCHfdUn8CRwbdRyrYy3aN6VwMpgulc7AawDxqYhXqCbXAKyC+hoVf0m1YUSHgihPUFZKpnZKOBcYCMwxN0/BQh+n9C6yEr8HPhn4C/B678FvnD3b4PXaavn7wMHgP8Mun2eMrNBpLSO3X0v8G/ksqxPgUPAO6S7jqF8fWbhOLwJWBtMpzJeM+sC9rr75qJZicebVANuEWWpvH/RzL4LrAJmu/ufWx1POWY2Bfjc3d8pLI5YNE31fAzQCSxx93PJjYuTiu6SKEHf8U+AU4ATgUHkTpOLpamOK0n1/mFm88h1Za4MiyIWa2m8ZvYdYB5wT9TsiLKmxptUA74HGFnwegSwL6FtV83M+pNrvFe6+3NB8X4zGxbMHwZ83qr4ilwMdJnZLuCX5LpRfg4MNrNwkLK01fMeYI+7bwxed5Nr0NNax/8A/N7dD7j7YeA54O9Jdx1D+fpM7XFoZtOBKcBPPeh/IJ3xnkruH/rm4NgbAbxrZkNpQbxJNeBvA6ODq/cDyF2YWJ3QtqtiZgYsAz5y94UFs1YD04Pp6eT6xlvO3ee6+wh3H0WuPn/j7j8FNgBXBYulJl4Ad/8M2G1mpwdFE4CtpLSOyXWdXGRm3wn2jzDe1NZxoFx9rgauD+6WuAg4FHa1tJKZXQHcDXS5+/8VzFoNXGNmA83sFHIXB3/bihhD7v6Bu5/g7qOCY28P0Bns28nXb4IXAiaTu8L8MTAv6QsRVcQ3jtzpzhbg/eBnMrl+5fXAjuD38a2ONSL28cCaYPr75HbyncCzwMBWx1cU64+ATUE9/w/wvTTXMfCvwDbgQ+AZYGCa6hj4Bbn++cPkGpMZ5eqT3Cn+4uAY/IDc3TVpiHcnub7j8Lj7j4Ll5wXx9gCT0hBv0fxdHLmImXj96lF6EZGM0pOYIiIZpQZcRCSj1ICLiGSUGnARkYxSAy4iklFqwEVEMkoNuIhIRv0/zLspoRG97EgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform.transforms.insert(\n",
    "    0, transforms.RandomRotation(degrees=(45, 90))\n",
    ")\n",
    "# Get a new iterator\n",
    "dataiter = iter(trainloader)\n",
    "im, lab = dataiter.next()\n",
    "\n",
    "image_show(\n",
    "    torchvision.utils.make_grid(im[0:SHOW_COUNT, :, :, :])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's reset the transforms. That is, remove the\n",
    "rotation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomRotation(degrees=(45, 90), resample=False, expand=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform.transforms.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten sufficiently side tracked with transforms, let's\n",
    "start making and training a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_channels: int):\n",
    "        super(Network, self).__init__()\n",
    "        self.convolution_1 = nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=20,\n",
    "            kernel_size=5, stride=1\n",
    "        )\n",
    "        self.max_pooling = nn.MaxPool2d(2, 2)\n",
    "        self.convolution_2 = nn.Conv2d(\n",
    "            in_channels=20, out_channels=50,\n",
    "            kernel_size=5, stride=1\n",
    "        )\n",
    "        # The math to compute this input dim is relatively\n",
    "        # simple. The formulas for H_out in both the pooling\n",
    "        # and the Conv are available on their respective pages.\n",
    "        self.dense_1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.dense_2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.max_pooling(F.relu(self.convolution_1(x)))\n",
    "        x = self.max_pooling(F.relu(self.convolution_2(x)))\n",
    "        # Flatten\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.log_softmax(self.dense_2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Network(input_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n",
      "\t[Batch 100] loss: 2.101159\n",
      "\t[Batch 200] loss: 0.894300\n",
      "\t[Batch 300] loss: 0.415393\n",
      "\t[Batch 400] loss: 0.343060\n",
      "\t[Batch 500] loss: 0.281044\n",
      "\t[Batch 600] loss: 0.250155\n",
      "\t[Batch 700] loss: 0.224295\n",
      "\t[Batch 800] loss: 0.225210\n",
      "\t[Batch 900] loss: 0.184700\n",
      "[Epoch 2]\n",
      "\t[Batch 100] loss: 0.172051\n",
      "\t[Batch 200] loss: 0.166591\n",
      "\t[Batch 300] loss: 0.141509\n",
      "\t[Batch 400] loss: 0.140838\n",
      "\t[Batch 500] loss: 0.137755\n",
      "\t[Batch 600] loss: 0.121129\n",
      "\t[Batch 700] loss: 0.132527\n",
      "\t[Batch 800] loss: 0.105698\n",
      "\t[Batch 900] loss: 0.116812\n",
      "[Epoch 3]\n",
      "\t[Batch 100] loss: 0.107528\n",
      "\t[Batch 200] loss: 0.090846\n",
      "\t[Batch 300] loss: 0.109447\n",
      "\t[Batch 400] loss: 0.093694\n",
      "\t[Batch 500] loss: 0.092258\n",
      "\t[Batch 600] loss: 0.089014\n",
      "\t[Batch 700] loss: 0.097224\n",
      "\t[Batch 800] loss: 0.073200\n",
      "\t[Batch 900] loss: 0.081627\n",
      "[Epoch 4]\n",
      "\t[Batch 100] loss: 0.085854\n",
      "\t[Batch 200] loss: 0.078553\n",
      "\t[Batch 300] loss: 0.068458\n",
      "\t[Batch 400] loss: 0.067084\n",
      "\t[Batch 500] loss: 0.068108\n",
      "\t[Batch 600] loss: 0.065419\n",
      "\t[Batch 700] loss: 0.069399\n",
      "\t[Batch 800] loss: 0.080481\n",
      "\t[Batch 900] loss: 0.065965\n",
      "[Epoch 5]\n",
      "\t[Batch 100] loss: 0.067769\n",
      "\t[Batch 200] loss: 0.063057\n",
      "\t[Batch 300] loss: 0.057506\n",
      "\t[Batch 400] loss: 0.060944\n",
      "\t[Batch 500] loss: 0.063097\n",
      "\t[Batch 600] loss: 0.057878\n",
      "\t[Batch 700] loss: 0.054606\n",
      "\t[Batch 800] loss: 0.058708\n",
      "\t[Batch 900] loss: 0.060125\n",
      "[Epoch 6]\n",
      "\t[Batch 100] loss: 0.060934\n",
      "\t[Batch 200] loss: 0.056048\n",
      "\t[Batch 300] loss: 0.048233\n",
      "\t[Batch 400] loss: 0.055156\n",
      "\t[Batch 500] loss: 0.054087\n",
      "\t[Batch 600] loss: 0.047103\n",
      "\t[Batch 700] loss: 0.051050\n",
      "\t[Batch 800] loss: 0.049591\n",
      "\t[Batch 900] loss: 0.051452\n",
      "[Epoch 7]\n",
      "\t[Batch 100] loss: 0.046676\n",
      "\t[Batch 200] loss: 0.045806\n",
      "\t[Batch 300] loss: 0.042078\n",
      "\t[Batch 400] loss: 0.058020\n",
      "\t[Batch 500] loss: 0.048822\n",
      "\t[Batch 600] loss: 0.045300\n",
      "\t[Batch 700] loss: 0.042555\n",
      "\t[Batch 800] loss: 0.038263\n",
      "\t[Batch 900] loss: 0.047923\n",
      "[Epoch 8]\n",
      "\t[Batch 100] loss: 0.037668\n",
      "\t[Batch 200] loss: 0.039474\n",
      "\t[Batch 300] loss: 0.044385\n",
      "\t[Batch 400] loss: 0.039609\n",
      "\t[Batch 500] loss: 0.037632\n",
      "\t[Batch 600] loss: 0.036498\n",
      "\t[Batch 700] loss: 0.045390\n",
      "\t[Batch 800] loss: 0.046380\n",
      "\t[Batch 900] loss: 0.042301\n",
      "[Epoch 9]\n",
      "\t[Batch 100] loss: 0.036330\n",
      "\t[Batch 200] loss: 0.038652\n",
      "\t[Batch 300] loss: 0.034303\n",
      "\t[Batch 400] loss: 0.030995\n",
      "\t[Batch 500] loss: 0.039247\n",
      "\t[Batch 600] loss: 0.036228\n",
      "\t[Batch 700] loss: 0.040465\n",
      "\t[Batch 800] loss: 0.036032\n",
      "\t[Batch 900] loss: 0.042628\n",
      "[Epoch 10]\n",
      "\t[Batch 100] loss: 0.034462\n",
      "\t[Batch 200] loss: 0.031993\n",
      "\t[Batch 300] loss: 0.030167\n",
      "\t[Batch 400] loss: 0.043933\n",
      "\t[Batch 500] loss: 0.029479\n",
      "\t[Batch 600] loss: 0.034394\n",
      "\t[Batch 700] loss: 0.033273\n",
      "\t[Batch 800] loss: 0.039502\n",
      "\t[Batch 900] loss: 0.026028\n"
     ]
    }
   ],
   "source": [
    "EPOCH_COUNT = 10\n",
    "STEPS_BETWEEN_PRINT = 100\n",
    "\n",
    "for epoch_index in range(EPOCH_COUNT):\n",
    "    print(f'[Epoch {epoch_index + 1}]')\n",
    "    running_loss = 0.0\n",
    "    for batch_index, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_index % STEPS_BETWEEN_PRINT == STEPS_BETWEEN_PRINT - 1:\n",
    "            print('\\t[Batch {}] loss: {:3f}'.format(\n",
    "                batch_index + 1,\n",
    "                running_loss / STEPS_BETWEEN_PRINT\n",
    "            ))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
