{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to start with MNIST, because it's a dataset that I'm\n",
    "massively familiar with that. On top of that, it has a continuous\n",
    "distribution, so once I get comfortable with implementing a simple\n",
    "convolutional neural network, I'll develop an auto encoder, and a \n",
    "variational auto encoder. \n",
    "\n",
    "For this example, we will do a simple convolutional neural network\n",
    "in torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchVision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [torchvision](https://pytorch.org/docs/stable/torchvision/index.html) \n",
    "package contains utilities for many tasks in computer vision. \n",
    "These include transformations, loading toy data sets, and popular model\n",
    "architectures. In this tutorial, I don't intend to use the last. I'll just \n",
    "[load](http://127.0.0.1:8888/?token=755449bfdb2fd6b486d94057b9759d4c877749c8b9a71482)\n",
    "MNIST (a popular dataset of images of numbers) and play about with it. \n",
    "\n",
    "In addition, we will start by just training the model without transformations. While\n",
    "this tutorial focuses on models in torch, it should be fun to see how transforms impact\n",
    "MNIST. Up front, I'll hypothesize that any gains will be very much marginal. The MNIST\n",
    "data set is pretty free of noise when preprocessed properly. Worth noting, there are \n",
    "plenty of datasets to play with, including QMNIST, which is not processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, we won't do any elaborate preprocessing. Instead, we just load the data\n",
    "and normalize.\n",
    "\n",
    "The image datasets are loaded as PILImages. We need to load these as tensors, and\n",
    "normalize them. For this, we can apply `transformation`s from `torchvision`.\n",
    "These \n",
    "[transforms](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision-transforms)\n",
    "are similar to the sklearn transforms, but are specifically for images. There is\n",
    "also a funcitonal module that allows for more custom control. \n",
    "\n",
    "Users can call the `Compose()` method to build pipelines of transformations. We \n",
    "just be casting the images to tensors, then centering the data ourselves. This \n",
    "way, we don't have to make any underlying assumptions about the distribution of\n",
    "the data. Worth noting that `Normalize` always expects an iterable. If you need\n",
    "to normalize single channel images, you must pass an iterable with one element. \n",
    "\n",
    "Note that mnist comes with a single (gray) channel of 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "trainset = MNIST(\n",
    "    root='./data/', train=True, \n",
    "    download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = trainset.data.float().mean() / 255\n",
    "sigma = trainset.data.float().std() / 255\n",
    "\n",
    "transform.transforms.append(\n",
    "    transforms.Normalize(\n",
    "        mean=(mu,), std=(sigma,)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(tensor(0.1307),), std=(tensor(0.3081),))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "im, lab = dataiter.next()\n",
    "\n",
    "im.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
