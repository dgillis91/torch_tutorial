{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to start with MNIST, because it's a dataset that I'm\n",
    "massively familiar with that. On top of that, it has a continuous\n",
    "distribution, so once I get comfortable with implementing a simple\n",
    "convolutional neural network, I'll develop an auto encoder, and a \n",
    "variational auto encoder. \n",
    "\n",
    "For this example, we will do a simple convolutional neural network\n",
    "in torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchVision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [torchvision](https://pytorch.org/docs/stable/torchvision/index.html) \n",
    "package contains utilities for many tasks in computer vision. \n",
    "These include transformations, loading toy data sets, and popular model\n",
    "architectures. In this tutorial, I don't intend to use the last. I'll just \n",
    "[load](http://127.0.0.1:8888/?token=755449bfdb2fd6b486d94057b9759d4c877749c8b9a71482)\n",
    "MNIST (a popular dataset of images of numbers) and play about with it. \n",
    "\n",
    "In addition, we will start by just training the model without transformations. While\n",
    "this tutorial focuses on models in torch, it should be fun to see how transforms impact\n",
    "MNIST. Up front, I'll hypothesize that any gains will be very much marginal. The MNIST\n",
    "data set is pretty free of noise when preprocessed properly. Worth noting, there are \n",
    "plenty of datasets to play with, including QMNIST, which is not processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, we won't do any elaborate preprocessing. Instead, we just load the data\n",
    "and normalize.\n",
    "\n",
    "The image datasets are loaded as PILImages. We need to load these as tensors, and\n",
    "normalize them. For this, we can apply `transformation`s from `torchvision`.\n",
    "These \n",
    "[transforms](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision-transforms)\n",
    "are similar to the sklearn transforms, but are specifically for images. There is\n",
    "also a funcitonal module that allows for more custom control. \n",
    "\n",
    "Users can call the `Compose()` method to build pipelines of transformations. We \n",
    "will just be casting the images to tensors, then centering the data ourselves. \n",
    "This way, we don't have to make any underlying assumptions about the distribution \n",
    "of the data. Worth noting that `Normalize` always expects an iterable. If you need\n",
    "to normalize single channel images, you must pass an iterable with one element. \n",
    "\n",
    "Note that mnist comes with a single (gray) channel of 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "trainset = MNIST(\n",
    "    root='./data/', train=True, \n",
    "    download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's possible to add transforms to a datasets transforms after the\n",
    "dataset has been constructed. This follows logically from how `Compose` is created.\n",
    "The `Compose` class is [defined](https://github.com/pytorch/vision/blob/19315e313511fead3597e23075552255d07fcb2a/torchvision/transforms/transforms.py#L43)\n",
    "with:\n",
    "\n",
    "```\n",
    "def __init__(self, transforms: list):\n",
    "    '''\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    '''\n",
    "    self.transforms = transforms\n",
    "```\n",
    "\n",
    "Because of this, we are free to append transformations to this object. The trainset\n",
    "that uses this will reference that object. \n",
    "\n",
    "So, in this case, we compute the mean and standard deviation so we can normalize the\n",
    "images with the parameters that define their distribution. We then `append` the \n",
    "created transformation to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = trainset.data.float().mean() / 255\n",
    "sigma = trainset.data.float().std() / 255\n",
    "\n",
    "transform.transforms.append(\n",
    "    transforms.Normalize(\n",
    "        mean=(mu,), std=(sigma,)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(tensor(0.1307),), std=(tensor(0.3081),))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=2\n",
    ")\n",
    "targets = np.array([x for x in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "im, lab = dataiter.next()\n",
    "\n",
    "im.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6 1 3 8 4 9 3 0 4\n"
     ]
    }
   ],
   "source": [
    "SHOW_COUNT = 10\n",
    "%matplotlib inline\n",
    "def image_show(image):\n",
    "    np_image = image.numpy()\n",
    "    plt.imshow(np.transpose(np_image, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "image_show(\n",
    "    torchvision.utils.make_grid(im[0:SHOW_COUNT, :, :, :])\n",
    ")\n",
    "print(' '.join(str(d) for d in targets[lab[:SHOW_COUNT]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
