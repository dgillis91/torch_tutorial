{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can use the `torch.nn` package to simplify the construction of \n",
    "neural networks. This package works similarly to Keras' subclassing\n",
    "API, but is simpler because it doesn't rely on the TF compute graph \n",
    "and is eager by default. You know, without requiring `tf.function`\n",
    "to be remotely performant on custom models. \n",
    "\n",
    "*Worth noting, it's also possible to use the Torch Sequential model, \n",
    "like Keras Sequential*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with an example. In Torch, models (and layers) are subclassed from\n",
    "`nn.Module`. For example, the class `MaxPool2d` \n",
    "[inherits](https://github.com/pytorch/pytorch/blob/1a74bd407de335019afdcb748a758107092a8019/torch/nn/modules/pooling.py#L79)\n",
    "from `nn.Module` via `_MaxPoolNd`. \n",
    "\n",
    "In this example, we define the layers in the `__init__` methods. We have:\n",
    "\n",
    "- `convolution_1`: Layer for one `input_channels` many input channels and\n",
    "6 output channels.\n",
    "- `convolution_2`: Layer for the 6 inputs and 16 outputs. \n",
    "\n",
    "Each convolution layer has a $3 \\times 3$ kernel.\n",
    "\n",
    "Then we construct the dense layers. \n",
    "\n",
    "- `dense_1`: There are 16 channels from the last convolution. In the toy example,\n",
    "each image is $6 \\times 6$ (with pooling). Thusly, we have $16 \\times 6 \\times 6$ input nodes. \n",
    "Finally, we have 120 outputs.\n",
    "- `dense_2`: 120 inputs and 84 outputs.\n",
    "- `classifier`: The final layer with the classification. \n",
    "\n",
    "Note that all of the `Linear` [layers](https://en.wikipedia.org/wiki/Affine_transformation) \n",
    "apply an [affine transform](https://en.wikipedia.org/wiki/Affine_transformation). In addition,\n",
    "the `Conv2d` [layers](keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "apply a 2D [convolution](https://en.wikipedia.org/wiki/Convolution) over an input plane.\n",
    "\n",
    "In the `forward` method, we compute the forward pass. This includes two \n",
    "[max pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling)\n",
    "operations. In addition, each convolution layer has a 'ReLU' activation applied. \n",
    "This is equivalent to:\n",
    "\n",
    "```\n",
    "model = Sequential([\n",
    "    Conv2d(input_channels, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2d(6, activation='relu'),\n",
    "    MaxPooling2d(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(120, activation='relu'),\n",
    "    Dense(84, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super(ConvolutionalNN, self).__init__()\n",
    "        # 6 output channels, 3x3 convolution.\n",
    "        self.convolution_1 = nn.Conv2d(input_channels, 6, 3)\n",
    "        # 6 input channels from the previous layer, 16 output channels.\n",
    "        self.convolution_2 = nn.Conv2d(6, 16, 3)\n",
    "        # Linear layers are affine transforms. No non-linearity.\n",
    "        # 16 out chans, 6x6 images. 120 outputs.\n",
    "        self.dense_1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.dense_2 = nn.Linear(120, 84)\n",
    "        self.classifier = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2x2 window\n",
    "        x = F.max_pool2d(F.relu(self.convolution_1(x)), (2, 2))\n",
    "        # If the window is square, you can specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.convolution_2(x)), 2)\n",
    "        # flatten\n",
    "        x = x.view(-1, self._num_flat_features(x))\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _num_flat_features(self, x):\n",
    "        # All sizes but batch\n",
    "        size = x.size()[1:]\n",
    "        feature_count = 1\n",
    "        for dim in size:\n",
    "            feature_count *= dim\n",
    "        return feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNN(\n",
      "  (convolution_1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (convolution_2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dense_1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (dense_2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (classifier): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the forward function, and autograd is able to supply the\n",
    "`backward` pass. We can then get the learnable parameters with \n",
    "`.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model Parameters:\n",
      "\t[+] Param Size 0: torch.Size([6, 1, 3, 3])\n",
      "\t[+] Param Size 1: torch.Size([6])\n",
      "\t[+] Param Size 2: torch.Size([16, 6, 3, 3])\n",
      "\t[+] Param Size 3: torch.Size([16])\n",
      "\t[+] Param Size 4: torch.Size([120, 576])\n",
      "\t[+] Param Size 5: torch.Size([120])\n",
      "\t[+] Param Size 6: torch.Size([84, 120])\n",
      "\t[+] Param Size 7: torch.Size([84])\n",
      "\t[+] Param Size 8: torch.Size([10, 84])\n",
      "\t[+] Param Size 9: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(f'[+] Model Parameters:')\n",
    "for index, parameter in enumerate(model.parameters()):\n",
    "    print(f'\\t[+] Param Size {index}: {parameter.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the components in the `nn` package expect data to be fed in batches. \n",
    "So, our model expects data of the form:\n",
    "\n",
    "`samples` $\\times$ `channels` $\\times$ `height` $\\times$ `width`.\n",
    "\n",
    "When you need to feed in a single sample, just wrap that sample in a fake batch. \n",
    "This can be done easily with `data.unsqueeze(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0634, -0.0135,  0.1940, -0.0105,  0.0673,  0.0047,  0.0130, -0.0793,\n",
      "          0.0876,  0.0252]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn((1, 1, 32, 32))\n",
    "out = model(data)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can zero the gradient buffers and run a backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "# backprop with random grads\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
