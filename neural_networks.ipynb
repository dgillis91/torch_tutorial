{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can use the `torch.nn` package to simplify the construction of \n",
    "neural networks. This package works similarly to Keras' subclassing\n",
    "API, but is simpler because it doesn't rely on the TF compute graph \n",
    "and is eager by default. You know, without requiring `tf.function`\n",
    "to be remotely performant on custom models. \n",
    "\n",
    "*Worth noting, it's also possible to use the Torch Sequential model, \n",
    "like Keras Sequential*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with an example. In Torch, models (and layers) are subclassed from\n",
    "`nn.Module`. For example, the class `MaxPool2d` \n",
    "[inherits](https://github.com/pytorch/pytorch/blob/1a74bd407de335019afdcb748a758107092a8019/torch/nn/modules/pooling.py#L79)\n",
    "from `nn.Module` via `_MaxPoolNd`. \n",
    "\n",
    "In this example, we define the layers in the `__init__` methods. We have:\n",
    "\n",
    "- `convolution_1`: Layer for one `input_channels` many input channels and\n",
    "6 output channels.\n",
    "- `convolution_2`: Layer for the 6 inputs and 16 outputs. \n",
    "\n",
    "Each convolution layer has a $3 \\times 3$ kernel.\n",
    "\n",
    "Then we construct the dense layers. \n",
    "\n",
    "- `dense_1`: There are 16 channels from the last convolution. In the toy example,\n",
    "each image is $6 \\times 6$ (with pooling). Thusly, we have $16 \\times 6 \\times 6$ input nodes. \n",
    "Finally, we have 120 outputs.\n",
    "- `dense_2`: 120 inputs and 84 outputs.\n",
    "- `classifier`: The final layer with the classification. \n",
    "\n",
    "Note that all of the `Linear` [layers](https://en.wikipedia.org/wiki/Affine_transformation) \n",
    "apply an [affine transform](https://en.wikipedia.org/wiki/Affine_transformation). In addition,\n",
    "the `Conv2d` [layers](keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "apply a 2D [convolution](https://en.wikipedia.org/wiki/Convolution) over an input plane.\n",
    "\n",
    "In the `forward` method, we compute the forward pass. This includes two \n",
    "[max pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling)\n",
    "operations. In addition, each convolution layer has a 'ReLU' activation applied. \n",
    "This is equivalent to:\n",
    "\n",
    "```\n",
    "model = Sequential([\n",
    "    Conv2d(input_channels, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2d(6, activation='relu'),\n",
    "    MaxPooling2d(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(120, activation='relu'),\n",
    "    Dense(84, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super(ConvolutionalNN, self).__init__()\n",
    "        # 6 output channels, 3x3 convolution.\n",
    "        self.convolution_1 = nn.Conv2d(input_channels, 6, 3)\n",
    "        # 6 input channels from the previous layer, 16 output channels.\n",
    "        self.convolution_2 = nn.Conv2d(6, 16, 3)\n",
    "        # Linear layers are affine transforms. No non-linearity.\n",
    "        # 16 out chans, 6x6 images. 120 outputs.\n",
    "        self.dense_1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.dense_2 = nn.Linear(120, 84)\n",
    "        self.classifier = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2x2 window\n",
    "        x = F.max_pool2d(F.relu(self.convolution_1(x)), (2, 2))\n",
    "        # If the window is square, you can specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.convolution_2(x)), 2)\n",
    "        # flatten\n",
    "        x = x.view(-1, self._num_flat_features(x))\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _num_flat_features(self, x):\n",
    "        # All sizes but batch\n",
    "        size = x.size()[1:]\n",
    "        feature_count = 1\n",
    "        for dim in size:\n",
    "            feature_count *= dim\n",
    "        return feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNN(\n",
      "  (convolution_1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (convolution_2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dense_1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (dense_2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (classifier): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the forward function, and autograd is able to supply the\n",
    "`backward` pass. We can then get the learnable parameters with \n",
    "`.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Model Parameters:\n",
      "\t[+] Param Size 0: torch.Size([6, 1, 3, 3])\n",
      "\t[+] Param Size 1: torch.Size([6])\n",
      "\t[+] Param Size 2: torch.Size([16, 6, 3, 3])\n",
      "\t[+] Param Size 3: torch.Size([16])\n",
      "\t[+] Param Size 4: torch.Size([120, 576])\n",
      "\t[+] Param Size 5: torch.Size([120])\n",
      "\t[+] Param Size 6: torch.Size([84, 120])\n",
      "\t[+] Param Size 7: torch.Size([84])\n",
      "\t[+] Param Size 8: torch.Size([10, 84])\n",
      "\t[+] Param Size 9: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(f'[+] Model Parameters:')\n",
    "for index, parameter in enumerate(model.parameters()):\n",
    "    print(f'\\t[+] Param Size {index}: {parameter.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the components in the `nn` package expect data to be fed in batches. \n",
    "So, our model expects data of the form:\n",
    "\n",
    "`samples` $\\times$ `channels` $\\times$ `height` $\\times$ `width`.\n",
    "\n",
    "When you need to feed in a single sample, just wrap that sample in a fake batch. \n",
    "This can be done easily with `data.unsqueeze(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1918, -0.2080,  0.1609,  0.1277, -0.0670,  0.0908,  0.0380, -0.1010,\n",
      "          0.0893, -0.0083]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn((1, 1, 32, 32))\n",
    "out = model(data)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can zero the gradient buffers and run a backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "# backprop with random grads\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions & Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch has multiple loss functions in the `nn` module. For \n",
    "[example](https://github.com/pytorch/pytorch/blob/cef0443464a4ff5e3fd2e3b6eca0ee76c5c428ce/torch/nn/functional.py#L2168), \n",
    "`nn.MSELoss()` \n",
    "[(mean squared error)](https://en.wikipedia.org/wiki/Mean_squared_error). \n",
    "[Loss Functions](https://github.com/pytorch/pytorch/blob/cef0443464a4ff5e3fd2e3b6eca0ee76c5c428ce/torch/nn/modules/loss.py#L8)\n",
    "in torch also extend the module class. Thusly, they define a `forward()` function,\n",
    "and then have a `backward` pass defined from it. This allows us to compute \n",
    "gradients W.R.T the loss. \n",
    "\n",
    "It's worth expanding on this a bit. When you allocate a loss function, you are\n",
    "creating an instance of `nn.Module`. From there, you can `__call__` the module.\n",
    "This computes the `forward` pass and creates the gradient graph, among other \n",
    "things. \n",
    "\n",
    "Now, the result of the forward pass is actually an operation, for which\n",
    "you can compute gradients. That is, when you compute the forward pass, MSELoss\n",
    "is delegating to `functional.mse_loss`\n",
    "[this](https://github.com/pytorch/pytorch/blob/cef0443464a4ff5e3fd2e3b6eca0ee76c5c428ce/torch/nn/functional.py#L2168)\n",
    "returns the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loss_value:\n",
      "0.74835604429245\n"
     ]
    }
   ],
   "source": [
    "output = model(data)\n",
    "target = torch.randn(10)\n",
    "target = target.view((1, -1))\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "loss = loss_function(output, target)\n",
    "print(f'[+] loss_value:\\n{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, the `__call__` to `loss_function` delegates to `forward`, which\n",
    "delegates to `functional.msel_loss` and returns an operation for which we can\n",
    "compute gradients via the `backward` method. \n",
    "\n",
    "In addition, we are also able to access the `grad_fn`, including its full trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fde5028aa90>\n",
      "((<AddmmBackward object at 0x7fde5028a8d0>, 0),)\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddmmBackward object at 0x7fdde5053350>\n",
      "<AccumulateGrad object at 0x7fdde5043350>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0][0]) # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, computing the `backward` pass is easy. But, it's important\n",
    "to call `zero_grad`, as torch accumulates the gradient buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] convolution_1.bias.grad:\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "[+] convolution_1.bias.grad:\n",
      "tensor([-0.0015, -0.0170,  0.0022, -0.0032,  0.0084, -0.0028])\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "\n",
    "print(f'[+] convolution_1.bias.grad:\\n{model.convolution_1.bias.grad}')\n",
    "\n",
    "# Recall that loss is tied to the outputs from the model\n",
    "# and the actual predictions. Made it hard for me to\n",
    "# remember how this is traced back.\n",
    "loss.backward()\n",
    "\n",
    "print(f'[+] convolution_1.bias.grad:\\n{model.convolution_1.bias.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization In Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by manually computing the gradient descent update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "for param in model.parameters():\n",
    "    # Subtraction in place\n",
    "    param.data.sub_(param.grad.data * LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be simplified with the\n",
    "[optim](https://pytorch.org/docs/stable/optim.html) package.\n",
    "Worth noting, this package supports some nice customization,\n",
    "such as varying learning rates by parameter (layer). In addition\n",
    "it's possible to start by training a subset of layers, then adding \n",
    "some from the optimizer.\n",
    "\n",
    "All optimizers inherit from the Torch `Optimizer` \n",
    "[class](https://github.com/pytorch/pytorch/blob/cef0443464a4ff5e3fd2e3b6eca0ee76c5c428ce/torch/optim/optimizer.py#L17)\n",
    ", which is in many cases abstract. \n",
    "\n",
    "To construct an optimizer, we pass in a set of parameters from our model.\n",
    "As a note from the documentation:\n",
    "\n",
    "```\n",
    "Parameters need to be specified as collections that have a deterministic ordering \n",
    "that is consistent between runs. Examples of objects that donâ€™t satisfy those \n",
    "properties are sets and iterators over values of dictionaries.\n",
    "```\n",
    "\n",
    "In the following example, we allocate an `SGD` optimizer. Next,\n",
    "we zero the gradient buffers. We then compute the forward pass,\n",
    "build the gradients WRT loss, and compute them. Finally, because\n",
    "the optimizer has access to the model parameters, it \"knows\" the\n",
    "gradients, and is able to apply them appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Always zero the gradient buffer . . . unless you are doing a custom \n",
    "# nesterov or momentum thing. But in that case, probably best to \n",
    "# just track your gradients. IDK. I'll ponder on it. Also probably\n",
    "# built ins for nesterov.\n",
    "optimizer.zero_grad()\n",
    "output = model(data)\n",
    "loss = loss_function(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
